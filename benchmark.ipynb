{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import attr\n",
    "from attr.validators import instance_of\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from typing import List, Dict, NewType, Callable, Any, Iterable, SupportsIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbenchs = Benchmarks((\"hola\", \"funcion_costosa\"))\\n\\n\\n\\n@benchs.dt(name=\"hola\")\\ndef sum_n_numbers(n: int):\\n    sum([random.randint(0,10) for _ in range(n)])\\n\\nfor _ in range(10000):\\n    sum_n_numbers(1000)\\n\\nasd = benchs.descriptions()'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_BenchName = NewType(\"bench_name\", str)\n",
    "T_dt = NewType(\"dt\", float)\n",
    "T_dts = NewType(\"dts\", List[T_dt])\n",
    "\n",
    "@attr.s(frozen=True)\n",
    "class BenchmarkDesc:\n",
    "    name: T_BenchName = attr.ib()\n",
    "    n_execs: int = attr.ib()\n",
    "    total_time: float = attr.ib()\n",
    "    mean: float = attr.ib()\n",
    "    median: float = attr.ib()\n",
    "    std: float = attr.ib()\n",
    "\n",
    "\n",
    "@attr.s(frozen=True)\n",
    "class BenchmarkData:\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "    -----------\n",
    "    - `name (str):` Nombre del benchmark.\n",
    "    - `dts (List[dt]):` Guarda el `dt` de cada ejecución.\n",
    "    - `n_execs (int):`  len(dts) ~~ Número de ejecuciones.\n",
    "    - `is_empty (bool):` n_execs==0 -> True.\n",
    "    - `total_time (float):` sum(dts) ~~ Suma de todos los tiempos de ejecución.\n",
    "    - `mean (float):`   np.mean(dts) ~~ Media.\n",
    "    - `median (float):` np.median(dts) ~~ Mediana.\n",
    "    - `std (float):` np.std(dts) ~~ Desv. Standard.\n",
    "    - `description (BenchmarkDesc)`: Retorna un diccionario con los valores representativos.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    \"\"\"\n",
    "    name: T_BenchName = attr.ib(validator=instance_of(str))\n",
    "    dts: T_dts = attr.ib(validator=instance_of(list), default=[])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dts)\n",
    "    \n",
    "    @property\n",
    "    def n_execs(self) -> int:\n",
    "        return len(self)\n",
    "    \n",
    "    @property\n",
    "    def is_empty(self) -> bool:\n",
    "        return self.n_execs == 0\n",
    "\n",
    "    @property\n",
    "    def total_time(self) -> float:\n",
    "        return self.__none_if_empty(np.sum(self.dts))\n",
    "    \n",
    "    @property\n",
    "    def mean(self) -> float:\n",
    "        return self.__none_if_empty(np.mean(self.dts))\n",
    "    \n",
    "    @property\n",
    "    def median(self) -> float:\n",
    "        return self.__none_if_empty(np.median(self.dts))\n",
    "    \n",
    "    @property\n",
    "    def std(self) -> float:\n",
    "        return self.__none_if_empty(np.std(self.dts))\n",
    "\n",
    "    @property\n",
    "    def description(self) -> BenchmarkDesc:\n",
    "        return BenchmarkDesc(\n",
    "            name = self.name,\n",
    "            n_execs = self.n_execs,\n",
    "            total_time = self.total_time,\n",
    "            mean = self.mean,\n",
    "            median = self.median,\n",
    "            std = self.std\n",
    "        )\n",
    "\n",
    "    def append(self, dt: T_dt) -> None:\n",
    "        self.dts.append(dt)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\" Elimina todos los `dts` guardados.\"\"\"\n",
    "        self.dts.clear()\n",
    "    \n",
    "    def __none_if_empty(self, value: Any) -> Any | None:\n",
    "        return value if not self.is_empty else None\n",
    "\n",
    "'''\n",
    "class Benchmarks(Dict[T_BenchName, BenchmarkData]):\n",
    "    \"\"\" Benchmark de todas las funciones decoradas con `dt`.\"\"\"\n",
    "    def __init__(self, bench_names: Iterable[str]):\n",
    "        assert len(bench_names) == len(set(bench_names)), \"No puede haber claves repetidas.\"\n",
    "        super().__init__({name: [] for name in bench_names})\n",
    "\n",
    "    @property\n",
    "    def time_now(self) -> float:\n",
    "        return time.perf_counter()\n",
    "\n",
    "    def dt(self, name: T_BenchName) -> Callable[..., Any]:\n",
    "        \"\"\"\n",
    "        Decorador: Calcula el tiempo que tarda en ejecutar la función decorada.\n",
    "        - name (str): Key donde se almacena \n",
    "        - Se almacena dentro de `Benchmark` con key `name` en una lista.\n",
    "        \"\"\"\n",
    "        def decorator(func: Callable[..., Any]) -> Callable[..., Any]:\n",
    "            def wrapper(*args, **kwargs):\n",
    "                t_i = self.time_now\n",
    "                result = func(*args, **kwargs)\n",
    "                dt = self.time_now - t_i\n",
    "                self.add_dt(name, dt)\n",
    "                return result\n",
    "            return wrapper\n",
    "        return decorator\n",
    "\n",
    "    def add_dt(self, bench_name: T_BenchName, dt: T_dt) -> None:\n",
    "        \"\"\" Appendea `dt` dentro de `bench_name`.\"\"\"\n",
    "        self[bench_name].append(dt)\n",
    "    \n",
    "    def description(self, bench_name: T_BenchName) -> BenchmarkData:\n",
    "        \"\"\" Retorna la descripción de un benchmark en concreto.\"\"\"\n",
    "        return BenchmarkData.from_dts(bench_name, self[bench_name])\n",
    "\n",
    "    def descriptions(self) -> List[BenchmarkData]:\n",
    "        \"\"\" Retorna la descripción de todos los benchmarks.\"\"\"\n",
    "        return [self.description(bench_name) for bench_name in self.keys()]\n",
    "\n",
    "    def reset(self, bench_name: T_BenchName = None):\n",
    "        \"\"\" \"\"\"\n",
    "        pass\n",
    "\n",
    "    def plot(self, bench_name: str) -> None:\n",
    "        \"\"\" TODO: Mejorar, no cuentro la forma rápida de hacer esto automático.\"\"\"\n",
    "        plt.hist(self[bench_name], log=True)\n",
    "        plt.show()'''\n",
    "'''\n",
    "benchs = Benchmarks((\"hola\", \"funcion_costosa\"))\n",
    "\n",
    "\n",
    "\n",
    "@benchs.dt(name=\"hola\")\n",
    "def sum_n_numbers(n: int):\n",
    "    sum([random.randint(0,10) for _ in range(n)])\n",
    "\n",
    "for _ in range(10000):\n",
    "    sum_n_numbers(1000)\n",
    "\n",
    "asd = benchs.descriptions()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkDesc(name='asdasd', n_execs=2, total_time=7, mean=3.5, median=3.5, std=0.5)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = BenchmarkData(\"asdasd\", [3,4])\n",
    "b.description"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
