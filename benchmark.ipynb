{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import attr\n",
    "from attr.validators import instance_of\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from typing import List, Dict, NewType, Callable, Any, Iterable, SupportsIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_BenchName = NewType(\"bench_name\", str)\n",
    "T_dt = NewType(\"dt\", float)\n",
    "T_dts = NewType(\"dts\", List[T_dt])\n",
    "\n",
    "\n",
    "@attr.s(frozen=True)\n",
    "class BenchDescription:\n",
    "    name: T_BenchName = attr.ib(validator=instance_of(str))\n",
    "    n_execs: int = attr.ib(validator=instance_of(int))\n",
    "    total_time: float = attr.ib(validator=instance_of(float | None))\n",
    "    mean: float = attr.ib(validator=instance_of(float | None))\n",
    "    median: float = attr.ib(validator=instance_of(float | None))\n",
    "    std: float = attr.ib(validator=instance_of(float | None))\n",
    "\n",
    "\n",
    "@attr.s(frozen=True)\n",
    "class BenchData:\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "    -----------\n",
    "    - `name (str):` Nombre del benchmark.\n",
    "    - `dts (List[dt]):` Guarda el `dt` de cada ejecución.\n",
    "    - `n_execs (int):`  len(dts) ~~ Número de ejecuciones.\n",
    "    - `is_empty (bool):` n_execs==0 -> True.\n",
    "    - `total_time (float):` sum(dts) ~~ Suma de todos los tiempos de ejecución.\n",
    "    - `mean (float):`   np.mean(dts) ~~ Media.\n",
    "    - `median (float):` np.median(dts) ~~ Mediana.\n",
    "    - `std (float):` np.std(dts) ~~ Desv. Standard.\n",
    "    - `description (BenchmarkDesc)`: Retorna un diccionario con los valores representativos.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    \"\"\"\n",
    "    name: T_BenchName = attr.ib(validator=instance_of(str))\n",
    "    dts: T_dts = attr.ib(validator=instance_of(list), factory=list, repr=False)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dts)\n",
    "    \n",
    "    @property\n",
    "    def n_execs(self) -> int:\n",
    "        return len(self)\n",
    "    \n",
    "    @property\n",
    "    def is_empty(self) -> bool: return self.n_execs == 0\n",
    "    @property\n",
    "    def not_empty(self) -> bool: return not self.is_empty\n",
    "\n",
    "    @property\n",
    "    def total_time(self) -> float | None:\n",
    "        if self.not_empty: return np.sum(self.dts)\n",
    "    \n",
    "    @property\n",
    "    def mean(self) -> float | None:\n",
    "        if self.not_empty: return np.mean(self.dts)\n",
    "    \n",
    "    @property\n",
    "    def median(self) -> float | None:\n",
    "        if self.not_empty: return np.median(self.dts)\n",
    "    \n",
    "    @property\n",
    "    def std(self) -> float | None:\n",
    "        if self.not_empty: return np.std(self.dts)\n",
    "\n",
    "    @property\n",
    "    def description(self) -> BenchDescription:\n",
    "        return BenchDescription(\n",
    "            name = self.name,\n",
    "            n_execs = self.n_execs,\n",
    "            total_time = self.total_time,\n",
    "            mean = self.mean,\n",
    "            median = self.median,\n",
    "            std = self.std\n",
    "        )\n",
    "\n",
    "    def append(self, dt: T_dt) -> None:\n",
    "        self.dts.append(dt)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\" Elimina todos los `dts` guardados.\"\"\"\n",
    "        self.dts.clear()\n",
    "    \n",
    "    def plot(self, log=False, *args, **kwargs):\n",
    "        plt.hist(self.dts, log=log, *args, **kwargs)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class Benchmarks(Dict[T_BenchName, BenchData]):\n",
    "    \"\"\" Benchmark de todas las funciones decoradas con `dt`.\"\"\"\n",
    "    def __init__(self, bench_names: Iterable[str]):\n",
    "        assert len(bench_names) == len(set(bench_names)), \"No puede haber claves repetidas.\"\n",
    "        super().__init__({name: BenchData(name=name) for name in bench_names})\n",
    "        \n",
    "        # Va a guardar el nombre del bench que está calculando.\n",
    "        # Para evitar el problema de los benchs anidados. Que una función llama a otra\n",
    "        # Y la suma de los tiempos no representa el tiempo total.\n",
    "        # Ver si tiene sentido implementarlo. O mejor un Warning, pero relentizaría.\n",
    "        # self.name_calculating: str | None = None\n",
    "\n",
    "    @property\n",
    "    def time_now(self) -> float:\n",
    "        return time.perf_counter()\n",
    "\n",
    "    def dt(self, name: T_BenchName) -> Callable[..., Any]:\n",
    "        \"\"\"\n",
    "        Decorador: Calcula el tiempo que tarda en ejecutar la función decorada.\n",
    "        - name (str): Key donde se almacena \n",
    "        - Se almacena dentro de `Benchmark` con key `name` en una lista.\n",
    "        \"\"\"\n",
    "        def decorator(func: Callable[..., Any]) -> Callable[..., Any]:\n",
    "            def wrapper(*args, **kwargs):\n",
    "                t_i = self.time_now\n",
    "                # Codigo antes de ejecutar func\n",
    "                result = func(*args, **kwargs)\n",
    "                # Codigo antes de despues func\n",
    "                dt = self.time_now - t_i\n",
    "                self.add_dt(name, dt)\n",
    "                return result\n",
    "            return wrapper\n",
    "        return decorator\n",
    "\n",
    "    def add_dt(self, bench_name: T_BenchName, dt: T_dt) -> None:\n",
    "        \"\"\" Appendea `dt` dentro de `bench_name`.\"\"\"\n",
    "        self[bench_name].append(dt)\n",
    "    \n",
    "    def description(self, bench_name: T_BenchName) -> BenchDescription:\n",
    "        \"\"\" Retorna la descripción de un benchmark en concreto.\"\"\"\n",
    "        return self[bench_name].description\n",
    "\n",
    "    def descriptions(self) -> Dict[T_BenchName, BenchDescription]:\n",
    "        \"\"\" Retorna la descripción de todos los benchmarks.\"\"\"\n",
    "        return {bench_name: self.description(bench_name) for bench_name in self.keys()}\n",
    "\n",
    "    def reset(self, bench_name: T_BenchName = None) -> None:\n",
    "        \"\"\" Resetea todos los `dts` si no se especifica uno en particular.\"\"\"\n",
    "        if bench_name is None:\n",
    "            for bench_data in self.values():\n",
    "                bench_data.reset()\n",
    "        else:\n",
    "            assert isinstance(bench_name, str)\n",
    "            self[bench_name].reset()\n",
    "\n",
    "bench = Benchmarks((\"f_1\", \"f_2\"))\n",
    "\n",
    "@bench.dt(name=\"f_1\")\n",
    "def func_1():\n",
    "    return random.random()*1000\n",
    "\n",
    "@bench.dt(name=\"f_2\")\n",
    "def func_2():\n",
    "    return np.log(func_1())\n",
    "\n",
    "for _ in range(100):\n",
    "    func_1()\n",
    "    func_2()\n",
    "\n",
    "# FIXME: Por alguna razón la primera vez que se accede al wrapper tarda levemente mas.\n",
    "# Capás es alguna tool de python, que hace algo la primera vez que agrega un elemento a una lista, o algo x el estilo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
